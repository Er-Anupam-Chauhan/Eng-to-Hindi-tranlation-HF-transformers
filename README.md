# Eng-to-Hindi-tranlation-HF-transformers

**Hugging Face**


****What is Hugging Face Transformers?****

Hugging Face Transformers is an open-source Python library that provides access to thousands of pre-trained Transformers models for natural language processing (NLP), computer vision, audio tasks, and more. It simplifies the process of implementing Transformer models by abstracting away the complexity of training or deploying models in lower level ML frameworks like PyTorch, TensorFlow and JAX.
My Understanding - (Just a library with pre-trained models in it)



**What is the Hugging Face Hub?**

The Hugging Face Hub is a collaboration platform that hosts a huge collection of open-source models and datasets for machine learning, think of it being like GitHub for ML. The hub facilitates sharing and collaborating by making it easy for you to discover, learn, and interact with useful ML assets from the open-source community. The hub integrates with, and is used in conjunction with the Transformers library, as models deployed using the Transformers library are downloaded from the hub.
My Understanding - (Hosting place for pre-trained models just like GitHub for pre-trained models)



**What are Hugging Face Spaces?**

Spaces from Hugging Face is a service available on the Hugging Face Hub that provides an easy to use GUI for building and deploying web hosted ML demos and apps. The service allows you to quickly build ML demos, upload your own apps to be hosted, or even select a number of pre-configured ML applications to deploy instantly.
My Understanding - (Just a workspace on the web to play around these models)

**How Transformers came into existence? What was the need to use transformers? And before transformers what were we using?**

Before diving into the core concept of transformers, let’s briefly understand what recurrent models are and their limitations.
Recurrent networks employ the encoder-decoder architecture, and we mainly use them when dealing with tasks where both the input and outputs are sequences in some defined ordering. Some of the greatest applications of recurrent networks are machine translation and time series data modelling.
Challenges with recurrent networks 
Let’s consider translating the following French sentence into English. The input transmitted to the encoder is the original French sentence, and the translated output is generated by the decoder.


![image](https://github.com/user-attachments/assets/0ddf8a5b-ab15-4bf1-a05b-307dde604448)

 
**A simple illustration of the recurrent network for language translation**
•	The input French sentence is passed to the encoder one word after the other, and the word embeddings are generated through the decoder in the same order, which makes them slow to train.
•	The current word's hidden state depends on the previous words' hidden states, which makes it impossible to perform parallel computation no matter the computation power used. 
•	Sequence to sequence neural networks is prompt to exploding gradients when the network is too long, which leads them to perform poorly. 
•	Long Short Terms Memory (LSTM) networks, which are other types of recurrent networks, have been introduced to mitigate the vanishing gradient, but these are even slower than sequence models. 
Wouldn't it be great to have a model that combines the benefits of recurrent networks and make parallel computation possible? 
This is the reason why Transformers came into existence



**My info Summary:**

1.	Hugging face is just a library with pre-trained models in it
2.	Hugging face hub is hosting place for pre-trained models just like GitHub for pre-trained models
3.	Hugging face space is just a workspace on the web to play around these models same like a virtual machine
4.	Hugging face also provides GPU’s on rent if your system is not having any
5.	Due to drawbacks of RNN like sequential processing of words (current words hidden state is dependent on the previous words hidden state making it as a sequential dependence of execution) Transformers came into picture offering parallel processing of the words with combined benefits of RNN
6.	In order to you hugging face functionality you have to install transformers




